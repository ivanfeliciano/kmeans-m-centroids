\documentclass[runningheads]{llncs}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{A greedy set cover approach for the recomputation of the cluster centres in the FPAC Algorithm}

\titlerunning{Set cover approach for the recomputation of the cluster centres}
\author{Ivan Feliciano\inst{1} \and
Edgar Hernandez-Gonzalez\inst{2}}
%
%\authorrunning{I. Feliciano et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{National Institute of Astrophysics, Optics and Electronics
\email{ivan.felavel@gmail.com}\\
\and
National Institute of Astrophysics, Optics and Electronics\\
\email{edgarmoy.28@gmail.com}}
\maketitle              % typeset the header of the contribution
\begin{abstract}
En este trabajo presentamos una modificación al algoritmo K-means usando una heurística que permite hacer el recalculo de centroides de una manera diferente al k-means tradicional. El procedimiento se basa en…primero, después, por ultimo.

\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
%
%
%
\section{Introduction}


The amount of information, specifically text, that is generated everyday
is increasing rapidly and dramatically. This necessitates application of effective and efficient content management techniques to fulfill the task of finding groups of similar documents in a collection of documents. This task is also known as 
clustering. These document clusters can then be useful for a variety
of applications, such as document alignment, information retrieval
(IR), text classification, etc \cite{ganguly_2018}.

We can define the goal in clustering as follows. 
Given a set of documents, $D = \{d_1 , \dots , d_N \}$,
a desired number of clusters, $K$,  and 
an objective function that evaluates the quality of a clustering.
we want to compute an assignment $\gamma : D \rightarrow \{1, \dots , K \}$ that minimizes (or, in other cases,
maximizes) the objective function. In most cases, we also demand that $\gamma$ is
surjective, that is, that none of the $K$ clusters is empty.
The objective function is often defined in terms of similarity or distance between documents.

$K$-means is perhaps the most widely used clustering algorithm
because of its simplicity and efficiency. The objective in $K$-means clustering is to minimize the average distance between documents and their
centroids or, equivalently, to maximize the similarity between documents
and their centroids.

$K$-means is an EM-based algorithm, in which starting with a set of randomly 
chosen initial centres, each input point is repeatedly assigned to its
nearest cluster centre, the E-step. The cluster centres are then recomputed by making use of the current assignments, the M-step.


The $K$-means clustering algorithm
does not scale well to datasets that are considerably large in size
and large in dimensionality, e.g. large document collections where
each document is a sparse vector of large dimension (vocabulary
size of the collection).
In the case of clustering vectors of large
dimensionality, the computational overhead of the $K$-means algorithm arises from both the E and the M steps, that is when: (a)
assigning vectors to one of the centres (interchangeably referred to
as ‘cluster centres’); and (b) recomputing the centres. Concretely
speaking, the computation required to assign each vector in the
collection to one of the centroids is expensive because it involves
computing the similarity of this data point with every cluster cen-
tre, which is an expensive operation if the collection is very large.

Since the introduction to the $K$-means algorithm, research
has progressed towards making it more efficient for cases where
the number of data points or the dimensionality of the data is
large, e.g. the case when the dataset is comprised of a collection
of documents.

The Fast Partitional Clustering Algorithm (FPAC) \cite{ganguly_2018}
involves the nearest neighbour based heuristic to
scale up $K$-means clustering for large document collections.
The main contribution of the FPAC on the $K$-means algorithm
is the use of an operation, namely $TOP(x)$ which gives a list
of the top mos similar vectors with respect to the current vector.
This operation is used in the three fundamental steps of the 
$K$-mean, the selection of the initial cluster centres,
the assignment of a cluster to a non-centroid vector $d$, and 
the recomputation of the centre clusters.

Our work is based on the FPAC algorithm and we have focused in the recomputation of the centroids step.
In the FPAC algorithm they select as the cluster centroid the 
vector with the highest number of distinct terms. Despite the efficiency
of just take the vector with the maximum number of unique terms,
what happen if the set of terms in the vector is disjoint from 
the other set of terms in the cluster vocabulary. That 
case inspired us to develop a solution based on the set cover problem.

Our contribution is the extension of the method for the recomputation of  the cluster centroids. Instead of using just one centroid that tries to
cover most of the vocabulary within a cluster, we proposed the selection
of $M$ centroids such that they cover the maximum number of terms 
in a cluster vocabulary.










%\begin{itemize}
%
%\item El número de documentos que se producen hoy en día
%\item Escribir el problema de agrupamiento
%\item Escribir para qué sirve el agrupamiento
%\item Describir el algoritmo Kmeans
%
%\end{itemize}
%
%
%
%
%Actualmente el numero de documentos en la web aumenta rápidamente, por tal motivo se necesitan algoritmos capaces de agrupar automáticamente grandes cantidades de datos.
%K-means es un algoritmo de agrupamiento, su objetivo es particionar un conjunto de datos en k grupos basándose en sus características. El agrupamiento se realiza minimizando la suma de distancias entre cada objeto y el centroide de su grupo \cite{ganguly_2018}.
%El algoritmo consta de tres pasos:
%1.	Inicialización: una vez escogido el número de grupos, k, se establecen k centroides en el espacio de los datos, por ejemplo, escogiéndolos aleatoriamente.
%2.	Asignación objetos a los centroides: cada objeto de los datos es asignado a su centroide más cercano.
%3.	Actualización centroides: se actualiza la posición del centroide de cada grupo tomando como nuevo centroide la posición del promedio de los objetos pertenecientes a dicho grupo.
%Se repiten los pasos 2 y 3 hasta que los centroides no se mueven, o se mueven por debajo de una distancia umbral en cada paso.
%A pesar de que el algoritmo K-means es muy popular no es escalable para datos de gran tamaño y dimensión.
%El principal cuello de botella de K-means es asignar cada vector no centroide a un grupo.

\section{Related Work}

The Fast PArtitional Clustering (FPAC) algorithm makes use of the
nearest neighbour based heuristic. This heuristic consists in using
a set of the most similar vector from a each cluster vector $x$. 
The operation of getting the set of the top nearest vectors 
is called $TOP(x)$ and is efficiently computated by using a 
inverted index data structure.

In a very general way we can hightlight three fundamental
steps in the FPAC algorithm: the selection of the initial centroids, the assinment of a cluster to a non-centroid vector and the recomputation of the cluster centres.

To ensure that the initially selected cluster centres are dissimilar
to each other, they select the first cluster centre, $C_1$, randomly from
the whole collection. The next cluster centroid, $C_2$, is then selected
such that it has a low similarity with the already chosen centroid.
More precisely, $C_2$ it is a randomly chosen vector from the collection
that is composed by the vectors that do not occur in the retreived list of 
$C_1$, $TOP(C_1)$. For selecting $C_3$, a vector is chosen at random that
does not occur in the union of the retrieved lists for $C_1$ and $C_2$, and
the process continues till $K$ cluster centres are obtained.

To assign a cluster to a non-centroid vector $d$, they use the $K$ centroids
as queries. If the cluster centres themselves are dissimilar to
each other, it is expected that the ranked lists retrieved for each
centre will have a small intersection. If a vector $d$ it is included it
is retrieved in the top set of only a single ranked list, $TOP(C_k)$,
it is included in the set of vector assigned to the cluster corresponding
to $C_k$. If a vector $d$ is retrieved in multiple ranked lists, 
it is assigned to the cluster for which the normalized
similarity score is maximum. The vectors which are not retrieved in the
union set of the ranked lists for each 
centre are assigned randomly to any one of the $K$ cluster centroids.

For computing the new centroid for a cluster they treat the vector 
which has the maximum number of unique terms within a cluster (i.e. 
the one with maximum length) as the updated cluster centre.


The results they obtained from the experiments of the FPAC algorithm
compared to the K-means and SK-means algorithms,
show an increase in the execution speed attributed to the IR-based approach
and the two additional heuristics related to initial centroid selection and 
centroid recomputation.

The FPAC similarity-based initial centroids selection
produces the best results in terms of clustering effectiveness.




%
%
%\begin{itemize}
%\item Describir los intentos hasta lo que hizo el autor
%\item Describir el algoritmo del autor
%\item Describir el problema con el algoritmo del autor
%\end{itemize}
%
%
%Se han hecho varias aportaciones para mejorar el algoritmo k-means.
%[la del articulo] desarrollo un algoritmo de partición rápida basado en una heurística de los vecinos mas cercanos. Dado un conjunto de centroides, evitar el calculo de distancia por pares entre vectores para obtener una partición de la colección, en su lugar se ocupó una asignación basada en el vecino mas cercano de cada centro, para esto se utilizo una lista invertida de vectores dispersos.
%También se evito el costoso calculo del centroide verdadero de cada grupo, se propuso una heurística para elegir el centroide de manera eficiente.
%En [9] los autores utilizan la heurística más lejana primero que consiste en seleccionar los centroides iniciales y evitar los cálculos de distancia redundante desde los no centroides a los centros.
%En [12, 19 y 20] se utilizaron estructuras de datos de partición de espacio como kd-trees. Esto aumenta la eficacia de k-means, pero solo para pocas dimensiones
%En [23] reasignaciones de clúster ocurren frecuentemente para puntos que no están cerca de los centroides. Identifica estos puntos al agrupar puntos vecinos usando múltiples arboles de partición.
%En [2 y 18] se utiliza una heurística la cual elije aleatoriamente el primer centroide y utiliza una distribución de probabilidad.
%En [4] se utilizo k-means escalable en el cual cada centroide se ve como una consulta para recuperar una lista de documentos que luego se asignan a ese grupo sin cálculo de distancia.
%[2] k-means ++.\cite{manning_2009}

\section{Proposed Solution}

\begin{itemize}
\item Describir en el paso en el algoritmo en que nos concentramos
\item Describir la adaptación que se hizo a los demás pasos para que utilizara varios
centroides, initCentroids() y getClosestCluster()
\item Tal vez sea bueno escribir un poco sobre el problema de ser cover y la aproximación greedy
\item introducir un poco de lo que se habla en la siguiente subsección
\end{itemize}

\subsection{Recomputation of the cluster centres}

\begin{itemize}
\item Describir el procedimiento
\item Tal vez la complejidad y desventajas
\end{itemize}

\section{Experiments}

\subsection{Dataset}

qué conjuntos de datos utilizamos

\begin{itemize}
\item Por qué no usamos el TREC Microblog Dataset
\item 20 news, descripción del dataset 
\item sentiment140 tweets
\item gender tweets
\end{itemize}

\subsection{Clustering evaluation}
\begin{itemize}
\item Medidas que utilizamos para evaluar el clustering y hablar un poco de las clases
que hay de cada conjunto que se utilizó
\item Purity
\item NMI
\item RI

\end{itemize}

\subsection{Implementation}

Decir que utilizamos como base lo desarrollado por Ganguly usando Lucene
\subsection{Compared approaches}

Decir que comparamos esta versión del FPAC M centroids con K means normal, SKMeans y FPAC.

\subsection{Parameters}



\begin{itemize}
\item El valor de K
\item el número de iteraciones
\item El valor de M
\end{itemize}

\section{Results}

\begin{itemize}
\item Por cada dataset una gráfica del NMI, Purity y RI
\item 
\end{itemize}
\section{Conclusions and future work}

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{references}
%
\end{document}
